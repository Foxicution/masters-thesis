{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from dateutil.parser import parse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from mt.definitions import DATA_DIR, REPO_DIR\n",
    "from mt.helper import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_TYPE_PATH = DATA_DIR / \"type_to_int.json\"\n",
    "\n",
    "\n",
    "def get_node_type_to_int() -> dict[str, int]:\n",
    "    if (path := EDGE_TYPE_PATH).exists():\n",
    "        with open(path) as f:\n",
    "            node_type_to_int = json.load(f)\n",
    "    else:\n",
    "        response = requests.get(\n",
    "            \"https://raw.githubusercontent.com/tree-sitter/tree-sitter-python/master/src/node-types.json\"\n",
    "        )\n",
    "        types = re.findall(r'\"type\": \"(.+)\"', response.text)\n",
    "        node_type_to_int = {t: i + 1 for i, t in enumerate(list(set(types)))}\n",
    "        with open(EDGE_TYPE_PATH, \"w\") as f:\n",
    "            json.dump(node_type_to_int, f)\n",
    "    return node_type_to_int\n",
    "\n",
    "\n",
    "node_type_to_int = get_node_type_to_int()\n",
    "edge_type_to_int = {\n",
    "    \"child\": 0,\n",
    "    \"occurance_of\": 1,\n",
    "    \"may_next_use\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "def get_commit_paths(repo: Path) -> dict[str, Path]:\n",
    "    commit_data_dir = repo / \"commit_data\"\n",
    "    commit_paths = list(commit_data_dir.glob(\"*.json\"))\n",
    "    commit_paths.sort(key=lambda path: int(path.name.split(\"_\")[0]))\n",
    "    commit_paths = {\n",
    "        path.name.split(\"_\")[1].removesuffix(\".json\"): path for path in commit_paths\n",
    "    }\n",
    "    return commit_paths\n",
    "\n",
    "\n",
    "def issue_open_at(issue: dict[str, Any], date: datetime) -> bool:\n",
    "    created_at = parse(issue[\"created_at\"])\n",
    "    closed_at = parse(issue[\"closed_at\"]) if issue[\"closed_at\"] else None\n",
    "    return created_at.replace(tzinfo=None) < date and (\n",
    "        not closed_at or closed_at.replace(tzinfo=None) > date\n",
    "    )\n",
    "\n",
    "\n",
    "def number_of_issues_open(issues: dict[str, Any], date: datetime) -> int:\n",
    "    return sum([1 for issue in issues if issue_open_at(issue, date)])\n",
    "\n",
    "\n",
    "def number_of_stars_at(stars: list[dict[str, Any]], date: datetime) -> int:\n",
    "    count = 0\n",
    "    for star in stars:\n",
    "        if parse(star[\"starred_at\"]).replace(tzinfo=None) <= date:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def nx_to_pyg_graph(nx_graph: nx.DiGraph) -> tuple[Data, dict[int, str]]:\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "    original_ids = {i: node for node, i in node_mapping.items()}\n",
    "\n",
    "    x = [\n",
    "        node_type_to_int.get(nx_graph.nodes[node][\"type\"], 0) for node in nx_graph.nodes\n",
    "    ]\n",
    "    x = torch.tensor(x, dtype=torch.long).unsqueeze(1)  # Shape (num_nodes, 1)\n",
    "\n",
    "    edge_list = []\n",
    "    edge_attr = []\n",
    "    for u, v, data in nx_graph.edges(data=True):\n",
    "        edge_list.append([node_mapping[u], node_mapping[v]])\n",
    "        edge_attr.append(edge_type_to_int[data[\"type\"]])\n",
    "    # edge_list = [[node_mapping[u], node_mapping[v]] for u, v in nx_graph.edges()]\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.long).unsqueeze(1)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr), original_ids\n",
    "\n",
    "\n",
    "def file_features_to_graph(\n",
    "    features: dict[str, str | bool | dict[str, int] | list[str]]\n",
    ") -> tuple[Data, dict[int, str]]:\n",
    "    with open(features[\"feature_file\"]) as f:\n",
    "        nx_graph = nx.node_link_graph(json.load(f)[\"ast\"])\n",
    "        return nx_to_pyg_graph(nx_graph)\n",
    "\n",
    "\n",
    "def fit_reg_calc_res(\n",
    "    no_issues: dict[str, int], no_stars: dict[str, int]\n",
    ") -> dict[str, int]:\n",
    "    residuals = {}\n",
    "    stars = np.array(list(no_stars.values())).reshape(-1, 1)\n",
    "    issues = np.array(list(no_issues.values()))\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(stars, issues)\n",
    "\n",
    "    predicted_issues = model.predict(stars)\n",
    "    residuals_list = issues - predicted_issues\n",
    "\n",
    "    for sha, residual in zip(no_stars.keys(), residuals_list):\n",
    "        residuals[sha] = residual\n",
    "\n",
    "    return residuals\n",
    "\n",
    "\n",
    "def process_commit(\n",
    "    commit: dict[str, Any],\n",
    "    path: Path,\n",
    "    idx: int,\n",
    "    pt_dir: Path,\n",
    "    issues: dict[str, Any],\n",
    "    stars: dict[str, Any],\n",
    ") -> tuple[str, int, int]:\n",
    "    \"returns msg, no_issues, no_stars\"\n",
    "    commit_date = parse(commit[\"commit\"][\"author\"][\"date\"]).replace(tzinfo=None)\n",
    "\n",
    "    # y_scale = number_of_stars_at(stars, commit_date)\n",
    "    # y = number_of_issues_open(issues, commit_date)\n",
    "    # y = y / (y_scale if y_scale else 1)\n",
    "    # y = y * 1000\n",
    "\n",
    "    with open(path) as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    maps, batch = [], []\n",
    "    for graph, id_map in [\n",
    "        file_features_to_graph(file_features) for file_features in raw_data.values()\n",
    "    ]:\n",
    "        maps.append(id_map)\n",
    "        batch.append(graph)\n",
    "\n",
    "    batch = Batch.from_data_list(batch)\n",
    "    torch.save((commit[\"sha\"], batch), pt_dir / f\"batch_{idx}.pt\")\n",
    "    with open(pt_dir / f\"map_{idx}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(maps, f)\n",
    "    return (\n",
    "        f\"{commit['sha']} Processed\",\n",
    "        number_of_issues_open(issues, commit_date),\n",
    "        number_of_stars_at(stars, commit_date),\n",
    "    )\n",
    "\n",
    "\n",
    "def process_repo(repo: Path) -> None:\n",
    "    with open(repo / \"stars.json\") as f:\n",
    "        stars = flatten([page[\"items\"] for page in json.load(f)])\n",
    "\n",
    "    with open(repo / \"commits.json\") as f:\n",
    "        commits = flatten([page[\"items\"] for page in json.load(f)])\n",
    "\n",
    "    with open(repo / \"issues.json\") as f:\n",
    "        issues = flatten([page[\"items\"] for page in json.load(f)])\n",
    "\n",
    "    pt_dir = repo / \"pts\"\n",
    "    pt_dir.mkdir(exist_ok=True)\n",
    "    commit_paths = get_commit_paths(repo)\n",
    "\n",
    "    all_issues, all_stars = {}, {}\n",
    "\n",
    "    futures, counter = {}, 0\n",
    "    for commit in commits:\n",
    "        if path := commit_paths.get(commit[\"sha\"]):\n",
    "            process_commit(commit, path, counter, pt_dir, issues, stars, maps)\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        sha = futures[future]\n",
    "        msg, no_issues, no_stars = future.result()\n",
    "        all_issues[sha] = no_issues\n",
    "        all_stars[sha] = no_stars\n",
    "        print(msg)\n",
    "\n",
    "    residuals = fit_reg_calc_res(all_issues, all_stars)\n",
    "\n",
    "    with open(pt_dir / \"no_stars.pkl\", \"wb\") as f:\n",
    "        pickle.dump(no_stars, f)\n",
    "\n",
    "    with open(pt_dir / \"no_issues.pkl\", \"wb\") as f:\n",
    "        pickle.dump(no_issues, f)\n",
    "\n",
    "    with open(pt_dir / \"residuals.pkl\", \"wb\") as f:\n",
    "        pickle.dump(residuals, f)\n",
    "\n",
    "\n",
    "\n",
    "repo = REPO_DIR / \"pytorch/vision\"\n",
    "process_repo(repo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
